# Street View Housing Number Digit Recognition

The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications. The Street View House Numbers (SVHN) dataset is one of the most popular ones. It has been used in neural networks created by Google to read house numbers and match them to their geolocations. This is a great benchmark dataset to play with, learn and train models that accurately identify street numbers, and incorporate into all sorts of projects.

#### Objective:
Given the testing and training data, can you train a model that accurately identifies house numbers in an image (with difficulties like picture brightness, blurriness)?

### Package version
- tensorflow==2.2.0
- matplotlib==3.2.1
- h5py==2.10.0
- google==2.0.3

# Importing the necessary Libraries
import h5py
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import Normalizer

h5f = h5py.File('/content/drive/My Drive/Colab Notebooks/SVHN_single_grey1.h5', 'r')

h5f.keys()

# Read the data from the h5py file and understand the train/test splits

### **NOTE** exchanging the validation and train dataset as validation dataset is large

#LOad the training, test and validation sets
X_train = h5f['X_val'][:]
y_train = h5f['y_val'][:]

X_val = h5f['X_train'][:]
y_val = h5f['y_train'][:]

X_test = h5f['X_test'][:]
y_test = h5f['y_test'][:]

# Checking the dimensions of the sets
X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape

# Checking the datatypes
type(X_train), type(y_train), type(X_val), type(y_val), type(X_test), type(y_test)

# There are 10 unique classes in the dataset
set(y_train)

# visualizing the first 10 images in the dataset and their labels
%matplotlib inline
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 1))
for i in range(10):
    plt.subplot(1, 10, i+1)
    plt.imshow(X_train[i], cmap="gray")
    plt.axis('off')
plt.show()
print('label for each of the above image: %s' % (y_train[0:10]))

# Reshape and normalize the train and test features

Need to reshape the X_train and X_test so that the same can be fed for model building. We need to feed a 2D tensor into the model and currently we have a 3D tensor. 

- We will normalize the data. We divide by 255 as this is a grayscale image and can take values from 0-255

# # changing the datatype to float32 as most of deep learning libraries uses this format

# X_train = X_train.astype('float32')
# y_train = y_train.astype('float32')
# X_val = X_val.astype('float32')
# y_val = y_val.astype('float32')
# X_test = X_test.astype('float32')
# y_test = y_test.astype('float32')

# One hot encode the labels for train and test data and validation data

y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)

# Define the model architecture using TensorFlow with a flatten layer followed by dense layers with activation as ReLu and softmax.  

### I have taken 4 hidden layers

#Initialize Sequential model
model = tf.keras.Sequential()

#Reshape data from 2D to 1D -> 32x32 to 1024
model.add(tf.keras.layers.Reshape((1024,),input_shape=(32,32,)))

#Normalize the data
model.add(tf.keras.layers.BatchNormalization())

#Add 1st hidden layer
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(256, kernel_initializer='he_normal', activation='relu'))
# model.add(tf.keras.layers.LeakyReLU())
model.add(tf.keras.layers.Dropout(0.05))

#Add 2nd hidden layer
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(64, kernel_initializer='he_normal', activation='relu'))
# model.add(tf.keras.layers.LeakyReLU())
model.add(tf.keras.layers.Dropout(0.4))

#Add 3rd hidden layer
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(64, kernel_initializer='he_normal', activation='relu'))
# model.add(tf.keras.layers.LeakyReLU())
model.add(tf.keras.layers.Dropout(0.2))

#Add 4rd hidden layer
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(32, kernel_initializer='he_normal', activation='relu'))
# model.add(tf.keras.layers.LeakyReLU())
model.add(tf.keras.layers.Dropout(0.2))

#Add OUTPUT layer
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(10, activation='softmax'))

#Learning Rate
adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=False)

# Compile the model
model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

# Fit and evaluate the model. Print the loss and accuracy for the test data

# Callbacks are used to save the best model during the training
model_checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_v1.h5', save_best_only=True, monitor='val_accuracy', mode='max')

model.fit(X_train,y_train,          
          validation_data=(X_val,y_val),
          epochs=100,
          batch_size=32,
          callbacks=[model_checkpoint])

# Save the model

model.save('boston_housing_lr.h5')

!ls -l

from google.colab import files

files.download('boston_housing_lr.h5')

# Fit and evaluate the model. Print the loss and accuracy for the test data

score, acc = model.evaluate(X_test, y_test, batch_size=32)

print("Test loss is:", score)
print('Test accuracy is:', acc)

X_train[0:1].shape

input_data = np.expand_dims(X_test[0], axis=0)

#Model prediction
prediction = model.predict(input_data)

#Print prediction
print(prediction)

#Get predicted number with highest probability
predicted_num = np.argmax(prediction[0])

#Print the number
print(predicted_num)

#Lets print the image as well
import matplotlib.pyplot as plt

plt.imshow(X_test[0],cmap='gray')
